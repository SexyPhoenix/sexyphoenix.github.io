{"meta":{"title":"SexyPhoenix的博客","subtitle":"博客","description":"这是一个分享技术的个人博客","author":"SexyPhoenix","url":"https://sexyphoenix.github.io"},"pages":[{"title":"","date":"2019-10-24T03:44:32.266Z","updated":"2019-10-24T03:44:32.267Z","comments":true,"path":"about/index.html","permalink":"https://sexyphoenix.github.io/about/index.html","excerpt":"","text":"张朋生 SexyPhoenixs（微信） | sexyphoenix@163.com | 南京GitHub: https://github.com/SexyPhoenix求职意向：高级PHP工程师、Python工程师 个人优势 本人有6年以上的web开发经验，现主要从事后端开发（可全栈），有独立设计与开发项目的工作能力，基础知识扎实，对PHP、前端、网络、Redis、Mysql以及分布式架构都有比较深的了解。新技术会实践并运用到项目中，喜欢看书（多方面）、锻炼以及分享。 工作经历 焦点科技股份有限公司（上市公司） 2014年6月 - 至今一、主要负责1、负责人力资源方向的信息化，参与项目的需求分析、产品规划、系统架构以及开发。2、负责企业内部信息化，主要有OA、考勤、商展以及手机版web等项目的需求分析以及开发。3、负责对以往项目的移植、优化以及重构。4、负责设计以及开发各应用的API接口。5、负责新员工的开发环境搭建以及后续技术支持。二、技能清单1、PHP（Laravel、Thinkphp等）、Reids、Mysql、Webpack、Grunt、Vue、Git等。2、网站架构：将应用和服务分层，分割并分布式部署、实现动静分离、加了中间缓存（Redis）、对数据库垂直分表分库、消息队列处理复杂功能三、成果实现了人力资源系统的闭环（招聘、入职、离职），主要功能是线上自动获取及分析各大网站简历落库、面试安排、简历登记、通知业务部门面试、审批、发送OFFER、入职签到、域账号自动生成以及离职权限统一释放等办公自动化功能。每日的PV达到2万+、简历数400左右。获得公司的优秀项目、个人优秀奖。南京天放网络科技有限公司 2014年1月 - 2014年6月一、自研TFWKCMS项目研发TFWKCMS项目，便于快速搭建网站1、参与项目需求分析和产品规划2、参与整个项目的架构搭建，实现了企业网站的绝大数功能。3、统一后台管理，将开发部分的周期缩小到一周以内。二、成果完成了快速搭建网站大多数功能（栏目、文章、权限、缓存、自定义前端等功能）无锡博志信息科技有限公司 2012年6月 - 2013年12月1、通过Dreamweaver、CSS、Jquery,制作HTML页面2、用PHPCMS进行网站的二次开发3、用ThinkPHP开发网销等网站 项目及作品 人力资源系统研发人力资源系统，主要目标是获取各大招聘网站投递的申请，实现人员入职离职自动化1、调查用户，分析项目的需求以及产品规划。2、解析焦点以及子公司邮箱简历，落库、生成申请、消息推送，最终完成招聘、入职等。3、开发web / 手机版面试登记（wepack / stylus / vue）。4、挖掘系统数据，进行分析以及图表可视化。5、开发人事基础服务以及API（基于OAuth2.0协议）。6、后期进行系统性能优化，实现秒开（前端 / 业务 / mysql等方面优化）。7、整个系统的PV量每日有20000+次。消息中心1、授权验证、推送、获取、完成代办、知晓代办等功能开发。2、消息落库、推送redis队列。3、websocket连接，监听频道等功能开发。4、消费进程监控。考勤系统1、分析项目的需求以及产品规划。2、设计系统架构，应对后续频繁的业务。3、开发审核流程、申请等功能。4、开发数据可视化等功能。5、优化功能以及操作细节。旅游项目vue的实践项目，技术栈（Webpack / Vue / Stylus）。1、幻灯片、推荐、周末去哪儿、城市选择以及字母联动、搜索等组件功能开发。2、vue router 实现路由功能。3、vuex 实现数据数据模块化。4、性能优化。高可用服务架构部署高可用实践项目1、Nginx 实现负载均衡。2、Envoy 实现代码同一部署。3、升级http到https。4、redis sentinel 实现会话保持。5、两台Mysql，实现主从架构（基于binlog）。6、Laravel实现读写分离。 教育经历 无锡科技职业学院 2010年9月 - 2013年6月软件技术 大专 无锡"},{"title":"categories","date":"2019-09-06T07:29:46.000Z","updated":"2019-09-06T07:29:46.685Z","comments":true,"path":"categories/index.html","permalink":"https://sexyphoenix.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Scrapy爬取豆瓣图书数据并写入MySQL","slug":"Scrapy爬取豆瓣图书数据并写入MySQL","date":"2019-11-11T03:35:26.000Z","updated":"2019-11-11T03:37:24.550Z","comments":true,"path":"Python/Scrapy爬取豆瓣图书数据并写入MySQL/","link":"","permalink":"https://sexyphoenix.github.io/Python/Scrapy爬取豆瓣图书数据并写入MySQL/","excerpt":"","text":"项目地址 BookSpider 介绍 本篇涉及的内容主要是获取分类下的所有图书数据，并写入MySQL 准备 Python3.6、Scrapy、Twisted、MySQLdb等 演示 代码 一、创建项目12scrapy startproject BookSpider #创建项目scrapy genspider douban book.douban.com #创建豆瓣爬虫 二、创建测试类（main.py）12from scrapy.cmdline import executeexecute([&apos;scrapy&apos;, &apos;crawl&apos;, &apos;douban&apos;]) 三、修改配置(spiders/settings.py)12USER_AGENT = &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36&apos; #浏览器ROBOTSTXT_OBEY = False #不遵循豆瓣网站的爬虫协议 四、设置爬取的分类（spiders/douban.py）1start_urls = [&apos;https://book.douban.com/tag/神经网络&apos;] # 只测试爬取神经网络 五、获取分类列表页图书数据 1234567891011121314from scrapy.http import Requestfrom urllib.parse import urljoindef parse(self, response): get_nodes = response.xpath(&apos;//div[@id=&quot;subject_list&quot;]/ul/li/div[@class=&quot;pic&quot;]/a&apos;) for node in get_nodes: url = node.xpath(&quot;@href&quot;).get() img_url = node.xpath(&apos;img/@src&apos;).get() yield Request(url=url, meta=&#123;&quot;img_url&quot;: img_url&#125;, callback=self.parse_book) # 传递img_url值 放在meta里面， parse_book回调函数，获取的详情再分析 next_url = response.xpath(&apos;//div[@class=&quot;paginator&quot;]/span[@class=&quot;next&quot;]/a/@href&apos;).get() # 获取下一页地址 if(next_url): yield Request(url=urljoin(response.url, next_url), callback=self.parse) # 获取下一页内容 六、定义数据模型（spiders/items.py）1234567891011121314class BookspiderItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() name = scrapy.Field() author = scrapy.Field() publish = scrapy.Field() page_num = scrapy.Field() isbm = scrapy.Field() binding = scrapy.Field() publish_date = scrapy.Field() price = scrapy.Field() rate = scrapy.Field() img_url = scrapy.Field() image_path = scrapy.Field() 七、获取图书详情数据 123456789101112131415161718192021222324import refrom BookSpider.items import BookspiderItemdef parse_book(self, response): BookItem = BookspiderItem() BookItem[&apos;name&apos;] = response.xpath(&apos;//span[@property=&quot;v:itemreviewed&quot;]/text()&apos;).get(&quot;&quot;).strip() BookItem[&apos;author&apos;] = response.xpath(&apos;//span[contains(text(), &quot;作者&quot;)]/following-sibling::a[1]/text()&apos;).get(&quot;&quot;).split()[-1] BookItem[&apos;publish&apos;] = response.xpath(&apos;//span[contains(text(), &quot;出版社&quot;)]/following-sibling::text()&apos;).get(&quot;&quot;).strip() page_num = response.xpath(&apos;//span[contains(text(), &quot;页数&quot;)]/following-sibling::text()&apos;).get(&quot;&quot;).strip() BookItem[&apos;page_num&apos;] = 0 if(page_num == &apos;&apos;) else page_num BookItem[&apos;isbm&apos;] = response.xpath(&apos;//span[contains(text(), &quot;ISBN&quot;)]/following-sibling::text()&apos;).get(&quot;&quot;).strip() BookItem[&apos;binding&apos;] = response.xpath(&apos;//span[contains(text(), &quot;装帧&quot;)]/following-sibling::text()&apos;).get(&quot;&quot;).strip() BookItem[&apos;publish_date&apos;] = response.xpath(&apos;//span[contains(text(), &quot;出版年&quot;)]/following-sibling::text()&apos;).get(&quot;&quot;).strip() price = response.xpath(&apos;//span[contains(text(), &quot;定价&quot;)]/following-sibling::text()&apos;).get(&quot;&quot;).strip() BookItem[&apos;price&apos;] = &apos;&apos; if(len(price) == 0) else re.findall(r&apos;\\d+\\.?\\d*&apos;, price)[0] BookItem[&apos;rate&apos;] = response.xpath(&apos;//div[contains(@class, &quot;rating_self &quot;)]/strong/text()&apos;).get(&quot;&quot;).strip() BookItem[&apos;img_url&apos;] = [response.meta.get(&apos;img_url&apos;)] #图片是列表 yield BookItem 八、下载图片1、创建images文件加2、配置spiders/settings.py 12345ITEM_PIPELINES = &#123; &apos;BookSpider.pipelines.ImageStorePipeline&apos;: 1, #后面的数据是优先级&#125;IMAGES_URLS_FIELD = &quot;image_url&quot;IMAGES_STORE = os.path.join(os.path.abspath(os.path.dirname(__file__)), &apos;images&apos;) 3、创建ImageStorePipeline类（spiders/pipelines.py） 123456789101112131415161718192021222324from scrapy.pipelines.images import ImagesPipelinefrom scrapy.exceptions import DropItemfrom scrapy.http import Requestclass ImageStorePipeline(ImagesPipeline): default_headers = &#123; &apos;accept&apos;: &apos;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3&apos;, &apos;accept-encoding&apos;: &apos;gzip, deflate, br&apos;, &apos;accept-language&apos;: &apos;zh-CN,zh;q=0.9&apos;, &apos;user-agent&apos;: &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36&apos;, #这个一定要 &#125; def get_media_requests(self, item, info): for image_url in item[&apos;img_url&apos;]: self.default_headers[&apos;referer&apos;] = image_url yield Request(image_url, headers=self.default_headers) def item_completed(self, results, item, info): image_path = [x[&apos;path&apos;] for ok, x in results if ok] if not image_path: raise DropItem(&quot;Item contains no images&quot;) item[&apos;image_path&apos;] = image_path return item 八、写入数据库1、配置spiders/settings.py 123456789#设置数据库MYSQL_HOST = &quot;&quot;MYSQL_DBNAME = &quot;&quot;MYSQL_USER = &quot;&quot;MYSQL_PASSWORD = &quot;&quot;ITEM_PIPELINES = &#123; &apos;BookSpider.pipelines.ImageStorePipeline&apos;: 1, &apos;BookSpider.pipelines.MysqlTwistedPipeline&apos;: 30, &#125; 2、创建MysqlTwistedPipeline类（spiders/pipelines.py） 12345678910111213141516171819202122232425262728import MySQLdb.cursorsfrom twisted.enterprise import adbapiclass MysqlTwistedPipeline(object): def __init__(self, dbpool): self.dbpool = dbpool @classmethod #静态方法，会优先执行from_settings， 这样self.dbpool就有值了 def from_settings(cls, settings): dbpool = adbapi.ConnectionPool(&quot;MySQLdb&quot;, host=settings[&apos;MYSQL_HOST&apos;], db = settings[&apos;MYSQL_DBNAME&apos;], user = settings[&apos;MYSQL_USER&apos;], passwd = settings[&apos;MYSQL_PASSWORD&apos;], charset = &apos;utf8&apos;, cursorclass = MySQLdb.cursors.DictCursor, use_unicode = True) return cls(dbpool) def process_item(self, item, spider): query = self.dbpool.runInteraction(self.do_insert, item) query.addErrback(self.handle_error,item,spider) def do_insert(self, cursor, item): insert_sql = &quot;&quot;&quot; insert into douban(name, author, publish, page_num, isbm, binding, publish_date, price, rate, img_url, image_path) values (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) &quot;&quot;&quot; cursor.execute(insert_sql, (item[&apos;name&apos;], item[&apos;author&apos;], item[&apos;publish&apos;], item[&apos;page_num&apos;], item[&apos;isbm&apos;], item[&apos;binding&apos;], item[&apos;publish_date&apos;], item[&apos;price&apos;], item[&apos;rate&apos;], item[&apos;img_url&apos;], item[&apos;image_path&apos;])) def handle_error(self, failure, item, spider): print(failure) 九、测试1、执行main.py文件","categories":[{"name":"Python","slug":"Python","permalink":"https://sexyphoenix.github.io/categories/Python/"}],"tags":[],"keywords":[{"name":"Python","slug":"Python","permalink":"https://sexyphoenix.github.io/categories/Python/"}]},{"title":"Python3获取豆瓣图书标签的前20本热门书籍（一）","slug":"DouBan","date":"2019-10-24T03:45:38.000Z","updated":"2019-10-31T06:10:48.641Z","comments":true,"path":"Python/DouBan/","link":"","permalink":"https://sexyphoenix.github.io/Python/DouBan/","excerpt":"","text":"介绍 第一篇主要获取豆瓣的大分类、大分类下的具体分类以及具体分类下的前20本热门书籍，第二篇对获取的数据进行分析。 准备Python3.6、requests、BeautifulSoup4 演示 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125# -*- coding: utf-8 -*-# @Author: Sexy Phoenix# @Last Modified by: Sexy Phoeniximport requestsfrom bs4 import BeautifulSoup, SoupStrainer#内容解析类class Parse: #解析分类 def parse_tags(self, content): only_div_tags = SoupStrainer(&apos;div&apos;, &apos;article&apos;) soup = BeautifulSoup(content, &apos;lxml&apos;, parse_only=only_div_tags) category = &#123;&#125; sub_category = &#123;&#125; # 解析大分类 tag_title_wrapper = soup.find_all(&apos;a&apos;, &apos;tag-title-wrapper&apos;) for index,tag in enumerate(tag_title_wrapper): category[index] = tag.get(&apos;name&apos;) # 解析大分类下的具体分类 tagCol = soup.find_all(&apos;table&apos;, &quot;tagCol&quot;) for i,tag in enumerate(soup.find_all(&apos;table&apos;, &quot;tagCol&quot;)): a = tag.find_all(&apos;a&apos;) sub_category[i] = [] for t in a: sub_category[i].append(t.string) return category, sub_category #解析具体分类前20分书籍 def parse_detail_tag(self, content): detail_conent = [] only_ul_tags = SoupStrainer(&apos;ul&apos;, &apos;subject-list&apos;) soup = BeautifulSoup(content, &apos;lxml&apos;, parse_only=only_ul_tags) for li in soup.find_all(&apos;li&apos;, &apos;subject-item&apos;): info = li.find(&apos;div&apos;, &apos;info&apos;) title = info.h2.a.get(&apos;title&apos;) star = info.find(&apos;span&apos;, &apos;rating_nums&apos;) extra_info = info.h2.next_sibling.next_sibling.string.split(&apos;/&apos;) author = extra_info[0].strip() price = extra_info[-1].strip() appraise = star.string appraise_num = star.next_sibling.next_sibling.string.strip() detail_conent.append(&#123; &apos;title&apos;: title, &apos;price&apos;: price, &apos;author&apos;: author, &apos;appraise&apos;:appraise, &apos;appraise_num&apos;: appraise_num &#125;) return detail_conent#内容获取类class Spider: def __init__(self): self.url = &apos;https://book.douban.com/tag/?view=type&amp;icn=index-sorttags-all&apos; self.tag_url = &apos;https://book.douban.com/tag/&apos; self.headers = &#123; &apos;User-Agent&apos; : &apos;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36&apos; &#125; self.parse = Parse() #获取分类HTML内容 def get_all_tag(self): data = requests.get(self.url, headers=self.headers) if(data.status_code == requests.codes.ok): return self.parse.parse_tags(data.text) else: print(&apos;[ERROR]: GET Category Error&apos;) #获取书籍HTML内容 def get_detail_tag(self, tag_name): data = requests.get(self.tag_url + tag_name, self.headers) if(data.status_code == requests.codes.ok): return self.parse.parse_detail_tag(data.text) else: print(&apos;[ERROR]: GET Sub Category Error&apos;) #显示 def show(self): category, sub_category = self.get_all_tag() print(&apos;豆瓣大分类：&apos;) for index,value in category.items(): i = index + 1 print(&quot;&#123;0&#125;、&#123;1&#125;&quot;.format(i, value)) try: key = int(input(&apos;请输入您选择的大分类：&apos;)) - 1 sub_cate = sub_category[key] for index in range(len(sub_cate)): i = index + 1 print(&quot;&#123;0&#125;、&#123;1&#125;&quot;.format(i, sub_cate[index])) try: sub_key = int(input(&apos;请输入您选择的具体分类：&apos;)) - 1 tag_name = sub_cate[sub_key] detail_content = self.get_detail_tag(tag_name) for book in detail_content: print(&apos;\\n&apos;) print(book[&apos;title&apos;]) print(&quot;作者：&#123;0&#125;, 价格：&#123;1&#125;, 评分：&#123;2&#125;&#123;3&#125;&quot;.format(book[&apos;author&apos;],book[&apos;price&apos;], book[&apos;appraise&apos;], book[&apos;appraise_num&apos;])) print(&apos;=&apos;*50) except: print(&apos;[ERROR]: 具体分类选择错误&apos;) except: print(&apos;[ERROR]: 大分类选择错误&apos;)#入口if __name__ == &apos;__main__&apos;: spider = Spider() spider.show()","categories":[{"name":"Python","slug":"Python","permalink":"https://sexyphoenix.github.io/categories/Python/"}],"tags":[],"keywords":[{"name":"Python","slug":"Python","permalink":"https://sexyphoenix.github.io/categories/Python/"}]},{"title":"Elastic Stack 开源的大数据解决方案","slug":"Elastic Stack 开源的大数据解决方案","date":"2019-09-06T09:04:24.000Z","updated":"2019-11-14T03:37:41.211Z","comments":true,"path":"数据库/Elastic Stack 开源的大数据解决方案/","link":"","permalink":"https://sexyphoenix.github.io/数据库/Elastic Stack 开源的大数据解决方案/","excerpt":"","text":"目的 本文主要介绍的内容有以下三点：一. Elastic Stack是什么以及组成部分二. Elastic Stack前景以及业务应用三. Elasticsearch原理（索引方向）四. Elasticsearch相对薄弱的地方 一、Elastic Stack是什么以及组成部分 介绍Elastic Stack是什么，其实只要一句话就可以，就是: 一套完整的大数据处理堆栈，从摄入、转换到存储分析、可视化。 它是不同产品的集合，各司其职，形成完整的数据处理链，因此Elastic Stack也可以简称为BLEK。 Beats 轻量型数据采集器 Logstash 输入、过滤器和输出 Elasticsearch 查询和分析 Kibana 可视化，可自由选择如何呈现数据 1. Beats - 全品类采集器，搞定所有数据类型Filebeat（日志文件）：对成百上千、甚至上万的服务器生成的日志汇总，可搜索。 Metricbeat（指标）: 收集系统和服务指标，CPU 使用率、内存、文件系统、磁盘 IO 和网络 IO 统计数据。 Packetbeat（网络数据）：网络数据包分析器，了解应用程序动态。 Heartbeat（运行时间监控）：通过主动探测来监测服务的可用性……Beats支持许许多多的beat，这里列的都是比较的常用的beat，了解更多可以点击链接：Beats 社区 2. Logstash - 服务器端数据处理管道介绍Logstash之前，我们先来看下Linux下常用的几个命令 1cat alldata.txt | awk ‘&#123;print $1&#125;’ | sort | uniq | tee filterdata.txt 只要接触过Linux同学，应该都知道这几个命名意思 1234cat alldata.txt #将cat alldata.txt的内容输出到标准设备上awk ‘&#123;print $1&#125;’ #对上面的内容做截取，只取每一行第一列数据sort | uniq #对截取后的内容，进行排序和唯一性操作tee filterdata.txt #将上面的内容写到filterdata.txt 上面的几个简单的命令就可以看出来，这是对数据进行了常规的处理，用名词修饰的话就是：数据获取/输入、数据清洗、数据过滤、数据写入/输出 而Logstash做的也是相同的事（看下图）。 将系统的日志文件、应用日志文件、系统指标等数据，输入到Input，再通过数据清洗以及过滤，输入到存储设备中，这里当然是输入到Elasticsearch 3. Elasticsearch - 分布式文档存储、RESTful风格的搜索和数据分析引擎Elasticsearch主要也是最原始的功能就是搜索和分析功能。这里就简单说一下，下面讲原理的时候会着重讲到Elasticsearch 搜索：全文搜索，完整的信息源转化为计算机可以识别、处理的信息单元形成的数据集合 。 分析：相关度，搜索所有内容，找到所需的具体信息（词频或热度等对结果排序） 4. Kibana- 可视化可视化看下图（来源官网）便知 可以对日志分析、业务分析等做可视化 现在从总体上来了解下，在心中对Elastic Stack有个清楚的认知（下图）。 二、Elastic Stack前景以及业务应用1. DB-Engines 排名 Elasticsearch是Elastic Stack核心，由图可以看出在搜索领域Elasticsearch暂时没有对手。 2. ES社区 ES中文社区也是相当活跃的，会定期做一下分享，都是大公司的宝贵经验，值得参考。 3. 2018年携程的使用情况（让我们看看能处理多大的数据）集群数量是94个，最小的集群一般是3个节点。全部节点数量大概700+。 最大的一个集群是做日志分析的，其中数据节点330个，最高峰一天产生1600亿文档，写入值300w/s。 现在有2.5万亿文档，大概是几个PB的量 三、Elasticsearch（ES）原理因为篇目有限，本篇只介绍ES的索引原理。 ES为什么可以做全文搜索，主要就是用了倒排索引，先来看下面的一张图 看图可以简单的理解倒排索引就是：关键字 + 页码 对倒排索引有个基本的认识后，下面来做个简单的数据例子。 现在对Name做到排索引，记住：关键字 + ID（页码）。 对Age做到排索引。 对Intersets做到排索引。 现在搜索Age等于18的，通过倒排索引就可以快速得到1和3的id，再通过id就可以得到具体数据，看，这样是不是快的狠。 如果是用Mysql等关系数据库，现在有十多亿数据（大数据嘛）,就要一条一条的扫描下去找id，效率可想而知。而用倒排索引，早到所有的id就轻轻松松了。 在ES中，关键词叫Term，页码叫Posting List。 但这样就行了吗？ 如果Name有上亿个Term，要找最后一个Term，效率岂不是还是很低？ 再来看Name的倒排索引，你会发现，将Armani放在了第一个，Tyloo放在了第三个，可以看出来，对Term做了简单的排序。虽然简单，但很实用。这样查找Term就可以用二分查找法来查找了，将复杂度由n变成了logn。 在ES中，这叫Term Dictionary。 到这里，再来想想MySQL的b+tree， 你有没有发现原理是差不多的，那为什么说ES搜索比MySQL快很多，究竟快在哪里？ 接下来再看。 有一种数据结构叫Trie树，又称前缀树或字典树，是一种有序树。这种数据结构的好处就是可以压缩前缀和提高查询数据。 现在有这么一组Term： apps, apple, apply, appear, back, backup, base, bear，用Trie树表示如下图。 通过线路路径字符连接就可以得到完成的Term，并且合用了前缀，比如apps, apple, apply, appear合用了app路径，节省了大量空间。 这个时候再来找base单词，立即就可以排除了a字符开头的单词，比Term Dictionary快了不知多少。 在ES中，这叫Term Index 现在我们再从整体看下ES的索引 先通过Trie树快速定位block（相当于页码）, 再到Term Dictionary 做二分查找，得到Posting List。 索引优化ES是为了大数据而生的，这意味着ES要处理大量的数据，它的Term数据量也是不可想象的。比如一篇文章，要做全文索引，就会对全篇的内容做分词，会产生大量的Term，而ES查询的时候，这些Term肯定要放在内存里面的。 虽然Trie树对前缀做了压缩，但在大量Term面前还是不够，会占用大量的内存使用，于是就有ES对Trie树进一步演化。 FST（Finite State Transducer ）确定无环有限状态转移器 （看下图） 可以看appear、bear 对共同的后缀做了压缩。 Posting List磁盘压缩假设有一亿的用户数据，现在对性别做搜索，而性别无非两种，可能”男”就有五千万之多，按int4个字节存储，就要消耗50M左右的磁盘空间，而这仅仅是其中一个Term。 那么面对成千上万的Term，ES究竟是怎么存储的呢？接下来，就来看看ES的压缩方法。 *Frame Of Reference （FOR） 增量编码压缩，将大数变小数，按字节存储 * 只要能把握“增量，大数变小数，按字节存储”这几个关键词，这个算法就很好理解，现在来具体看看。 现在有一组Posting List：[60, 150, 300,310, 315, 340], 按正常的int型存储，size = 6 * 4（24）个字节。 按增量存储：60 + 90（150）+ 150（300） + 10（310） + 5（315）+ 25（340），也就是[60, 90, 150, 10, 5, 25]，这样就将大数变成了小数。 切分成不同的block：[60, 90, 150]、[10, 5, 25]，为什么要切分，下面讲。 按字节存储：对于[60, 90, 150]这组block，究竟怎么按字节存储，其实很简单，就是找其中最大的一个值，看X个比特能表示这个最大的数，那么剩下的数也用X个比特表示（切分，可以尽可能的压缩空间）。 [60, 90, 150]最大数150 &lt; 2^8 = 256，也就是这组每个数都用8个比特表示，也就是 3*8 = 24个比特，再除以8，也就是3个字节存在，再加上一个8的标识位（说明每个数是8个比特存储）,占用一个字节，一共4个字节。 [10, 5, 25]最大数25 &lt; 2^5 = 32，每个数用5个比特表示，3*5=15比特，除以8，大约2个字节，加上5的标识位，一共3个字节。 那么总体size = 4 + 3（7）个字节，相当于24个字节，大大压缩了空间。 再看下图表示 Posting List内存压缩同学们应该都知道越复杂的算法消耗的CPU性能就越大，比如常见的https，第一次交互会用非对称密码来验证，验证通过后就转变成了对称密码验证，FOR同样如此，那么ES是用什么算法压缩内存中的Posting List呢？ Roaring Bitmaps 压缩位图索引 Roaring Bitmaps 涉及到两种数据结构 short[] 、bitmap。 short好理解就是2个字节的整型。 bitmap就是用比特表示数据，看下面的例子。 Posting List：[1, 2, 4, 7, 10] -&gt; [1, 1, 0, 1, 0, 0, 1,0, 0, 1]，取最大的值10，那么就用10个比特表示这组Posting List，第1, 2, 4, 7, 10位存在，就将相对应的“位”置为1，其他的为0。 但这种bitmap数据结构有缺陷，看这组Posting List： [1, 3, 100000000] -&gt; [1, 0, 1, 0, 0, 0, …, 0, 0, 1 ]，最大数是1亿，就要1亿的比特表示，这么算下来，反而消耗了更多的内存。 那如何解决这个问题，其实也很简单，跟上面一样，将大数变成小数。 看下图： 第一步：将每个数除以65536，得到（商，余数）。 第二步：按照商，分成不同的block，也就是相同的商，放在同一个block里面，余数就是这个值在这个block里面的位置（永远不会超过65536，余数嘛）。 第三步：判断底层block用什么数据结构存储数据，如果block里面的余数的个数超过4096个，就用short存储，反之bitmap。 上面那个图是官网的图，看下我画的图，可能更好理解些。 到这里，ES的索引原理就讲完了，希望大家都能理解。 四、Elasticsearch（ES）相对薄弱的地方1. 多表关联其实ES有一个很重要的特性这里没有介绍到，也就是分布式，每一个节点的数据和，才是整体数据。 这也导致了多表关联问题，虽然ES里面也提供了Nested&amp; Join 方法解决这个问题，但这里还是不建议用。 那这个问题在实际应用中应该如何解决？ 其实也很简单，装换思路，ES无法解决，就到其他层解决，比如：应用层，用面向对象的架构，拆分查询。 2. 深度分页分布式架构下，取数据便不是那么简单，比如取前1000条数据，如果是10个节点，那么每个节点都要取1000条，10个节点就是10000条，排序后，返回前1000条，如果是深度分页就会变的相当的慢。 ES提供的是Scroll + Scroll_after，但这个采取的是缓存的方式，取出10000条后，缓存在内存里，再来翻页的时候，直接从缓存中取，这就代表着存在实时性问题。 来看看百度是怎么解决这个问题的。 一样在应用层解决，翻页到一定的深度后，禁止翻页。 3. 更新应用频繁更新的应用，用ES会有瓶颈，比如一些游戏应用，要不断的更新数据，用ES不是太适合，这个看大家自己的应用情况。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://sexyphoenix.github.io/categories/数据库/"}],"tags":[],"keywords":[{"name":"数据库","slug":"数据库","permalink":"https://sexyphoenix.github.io/categories/数据库/"}]},{"title":"Git基础使用","slug":"Git基础使用","date":"2019-09-06T09:02:11.000Z","updated":"2019-09-09T01:38:30.526Z","comments":true,"path":"其他/Git基础使用/","link":"","permalink":"https://sexyphoenix.github.io/其他/Git基础使用/","excerpt":"","text":"前言 Git是版本控制系统，由Linux开源社区开发。与其他的版本系统相比，Git更加快速，便捷。主要是Git存储的是快照，而非差异性比较。并且绝大数操作都是访问本地文件和资源，没有网络时也可以直接提交，等到有网时再推送到远程仓库。对于文件的历史也是直接拉取本地，瞬间完成。 背景 解决一下场景遇到的问题业务：个人信息的需求。Coder：码码码码码码。。。（进行中）安全部门：怎么档案的信息改下id，能看到别人的档案，赶紧修复。Coder：个人信息还没做好，档案修复和个人信息文件又存在交叉，不能提交，该怎么办？ 文件状态变化周期 文件的状态只有两种：未跟踪（untracked）和已跟踪（unmodified、modified、staged） 1. 工作目录下创建new.php文件 执行 git status，可以发现new.php还没有被git跟踪 2. 跟踪new.php文件 执行git add . 后，文件被放入暂存区（staged） 3. 修改new.php的内容git status 后，出现 Changes not staged for commit，说明跟踪的文件已被修改，还未放入在暂存区 4. 暂存修改的new.phpgit add 是个多功能的命令，既可以将未跟踪的文件放入暂存区，也可以将修改的文件放入暂存区，当然它还有其他的一些功能。从上面的图我们可以看到，修改的文件被放入到暂存区了。 5. 提交，生成快照git commit -m “add new” 后，已生成此次的快照，校验和为 dd90005 note ： git commit -a -m ‘’add new”， 可以跳过git add 6. 删除提交git rm后，重新提交，文件在工作目录和暂存区中都被删除。 分支git的原理由5个对象实现，想知道具体的可以搜下资料看看，这里主要讲如何解决背景出现的问题。git log –oneline –decorate –graph –all //先执行此命令查看工作目录所处的分支 根据上图可以看到工作目录处于master分支，HEAD指向master分支，流程图： 1.个人信息需求git branch issue //此时HEAD还是指向master创建issue分支，这点很重要，当有新需求过来的时候，一定要创建自己的分支。保持主分支为原样。git checkout issue //切换到issue分支，HEAD指向issue 流程图： 2. 码码码码码码在issue分支下，工作，并提交到暂存区可以看到生成了校验码为c7abbef的快照，流程图： 3. 档案信息修复git checkout master //切换master主分支，HEAD指向mastergit branch issue2 //创建issue2分支，HEAD指向mastergit checkout issue2 //HEAD指向issue2修复bug，同提交，产生3a59570快照。此时流程图： git checkout master //切换master分支git merge issue2 //issue2分支的内容合并到master分支流程图：此时的合并只是将master指针前移。git branch -d issue2 //删除issue2分支 4. 继续个人信息git checkout issue //切换issue分支码码码码码。。。。功能实现继续提交流程图：git checkout master //切换到master主分支git merge issue //合并issue分支到master主分支 此时的合并就不是简单的将master指针前移，因为这两个分支的共同分支是9ffb7ee，而不是3a59570，此次合并做了两次操作，一是将94517a9、9ffb7ee、3a59570的结果做了一次新的快照，二是对结果做了一次新的提交10af497。 注：此时如果有文件冲突，出现 CONFLICT (content): Merge conflict ，可以到冲突的文件中，修改冲突的内容，再次 git commit -a -m “fix confilct” 关于git的使用就讲到这了，上面讲的这些也只是git的基本使用。当我们再去深入的了解的话，就会发现用git也可以实现运维系统发布那一套流程，每一个开发者将自己私有库的更新发布到自己的公共库上，再由管理者去拉取开发者的公共库更新，管理者发现没有问题，再推送到主仓库。","categories":[{"name":"其他","slug":"其他","permalink":"https://sexyphoenix.github.io/categories/其他/"}],"tags":[],"keywords":[{"name":"其他","slug":"其他","permalink":"https://sexyphoenix.github.io/categories/其他/"}]},{"title":"HTTPS部署","slug":"HTTPS部署","date":"2019-09-06T09:00:04.000Z","updated":"2019-09-06T09:00:47.335Z","comments":true,"path":"安全/HTTPS部署/","link":"","permalink":"https://sexyphoenix.github.io/安全/HTTPS部署/","excerpt":"","text":"前言 考虑到HTTP的安全性问题，现在很多网站已经将HTTP升级到了HTTP + SSL（HTTPS）。但也并不是所有的HTTPS站点就是安全的，也可能存在中间人的攻击（不是权威的CA机构颁发的证书以及证书校验不严格）。下图就是关于“中间人攻击”的原理图。不过权威CA机构颁发证书大多数是收费的，想用免费的可以考虑 Let’s Encrypt。什么才是权威呢？就是CA机构向浏览器厂商申请，申请通过后，由浏览器厂商将CA机构的根证书（简称CA证书）内嵌在浏览器中。也就是那些为企业签发证书的CA证书都是受浏览器信任的。 而证书一般有三种，根证书、服务器证书、客户端证书。根证书是生成服务器证书和客户端证书的基础，也就是CA证书。服务器证书是放在服务器上的，并引入到站点的配置文件中，由CA证书签名。相当于有一封信件（服务器证书），由CA盖章（签名），表示此信件受CA信任。客户端证书是对于个人的，这里不做演示。 这样就可以防御中间人攻击了，当客户端发起HTTPS请求时，服务器将服务器证书传给客户端，客户端用内嵌的CA证书和获取到的服务器证书做信息比较，如果发现是伪造的证书，客户端发出警告。 接下来就模拟下整个证书生成的环节，可以有一个清楚的认识。因为是本地环境，就自建CA根证书了（Let’s Encrypt 有域名验证之类的步骤）。 根证书（CA证书） 1openssl version -a //openssl所有安装信息 1cd /usr/lib/ssl 123cd /etc/ssl //到Linux专门的配置目录中设置CAmkdir req //放服务器证书mkdir newcert //放签名后的服务器证书 1cp openssl.cnf cacert.cnf 1vim cacert.cnf 修改v3_ca 下面设置项。 修改v3_req的设置项， DNS参数值为要升级为HTTPS的域名。开启 v3_req（去掉 #）。 生成根证书的私钥 1openssl genrsa -aes256 -out private/cakey.pem 2048 //用-aes256加密生成cakey.pem私钥，密码记住后面要用 生成根证书CA （自签） 1openssl req -new -x509 -subj &quot;/C=CN/CN=FocusChina Corporation Root CA/ST=JiangSu/L=NanJing/O=FocusChina/OU=FC&quot; -extensions v3_ca -days 3650 -key private/cakey.pem -sha256 -out cacert.pem -config cacert.cnf 查看CA证书 123openssl x509 -in cacert.pem -text -noout![6.png](https://upload-images.jianshu.io/upload_images/13834020-709773247db1696b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)Issuer与Subject一致 以上CA根证书建立完成，下面就可以给相应的服务器证书签名。 服务器证书 1cd /etc/nginx/ssl/ //这里将服务器证书放在 /etc/nginx/ssl 目录下。 生成服务器证书私钥 (www.app.goods) 1openssl genrsa -out www.app.goods.key 2048 生成服务器证书 1openssl req -subj &quot;/C=CN/CN=app.goods/ST=JiangSu/L=NanJing/O=FocusChina/OU=FC&quot; -extensions v3_req -sha256 -new -key www.app.goods.key -out www.app.goods.csr CN的值为服务器名，其他的和根证书保持一致。 1cp www.app.goods.csr /etc/ssl/req 服务器证书生成后，就可以将相关信息（公司信息、服务器证书，域名等）交给CA机构，CA机构会根据提供的信息去验证公司信息、域名是否属实。接下来，给服务器证书签名。 签名 在前面已经建立了自己的CA证书，下面就用生成的CA证书给服务器签名。 签名 1openssl x509 -req -extensions v3_req -days 3650 -sha256 -in ./req/www.app.goods.csr -CA cacert.pem -CAkey private/cakey.pem -CAcreateserial -out ./newcert/www.app.goods.crt -extfile cacert.cnf //用CA证书、CA私钥、服务器证书生成www.app.goods.crt，有效期10年 查看证书 1openssl x509 -in ./newcert/www.app.goods.crt -text -noout 1cp ./newcert/www.app.goods.crt /etc/nginx/ssl //将签名后的证书交给服务器 配置服务器 12cd /etc/nginx/conf.dvim www.app.goods.conf ssl 监听端口为443， 开启ssl，并加载服务器证书私钥以及证书。 1service nginx restart //重启服务 https://www.app.good //chrome 打开网站页面出现“您的连接不是私密连接”，是因为自建的根证书或者服务器证书不被浏览器信任。 导出根证书 12cd /etc/sslsz cacert.pem //发送到桌面。 Google 设置 高级 &gt; 管理证书 受信任的根证书颁发机构 &gt; 导入cacert.pem 运行 &gt; certmgr.msc //chrome用的是window系统的证书管理 刷新 https://www.app.goods/chrome、IE等已成功 Firefox 用的不是window系统的证书管理，需要导入到浏览器Firefox 选项 &gt; 隐私与安全 查看证书 &gt; 导入cacert.pem 证书颁发机构 （下载证书 勾选第一个框）至此，HTTPS部署成功 强制HTTP跳转 1service nginx restart //重启服务 访问 http://www.app.goods 会调整到 https://www.app.goods/","categories":[{"name":"安全","slug":"安全","permalink":"https://sexyphoenix.github.io/categories/安全/"}],"tags":[],"keywords":[{"name":"安全","slug":"安全","permalink":"https://sexyphoenix.github.io/categories/安全/"}]},{"title":"打开网站发生了什么？","slug":"打开网站发生了什么？","date":"2019-09-06T08:59:08.000Z","updated":"2019-09-06T08:59:27.014Z","comments":true,"path":"网络/打开网站发生了什么？/","link":"","permalink":"https://sexyphoenix.github.io/网络/打开网站发生了什么？/","excerpt":"","text":"敬请期待","categories":[{"name":"网络","slug":"网络","permalink":"https://sexyphoenix.github.io/categories/网络/"}],"tags":[],"keywords":[{"name":"网络","slug":"网络","permalink":"https://sexyphoenix.github.io/categories/网络/"}]},{"title":"索引优化","slug":"索引优化","date":"2019-09-06T08:57:51.000Z","updated":"2019-09-09T06:56:32.866Z","comments":true,"path":"数据库/索引优化/","link":"","permalink":"https://sexyphoenix.github.io/数据库/索引优化/","excerpt":"","text":"敬请期待","categories":[{"name":"数据库","slug":"数据库","permalink":"https://sexyphoenix.github.io/categories/数据库/"}],"tags":[],"keywords":[{"name":"数据库","slug":"数据库","permalink":"https://sexyphoenix.github.io/categories/数据库/"}]},{"title":"前端优化","slug":"前端优化","date":"2019-09-06T08:52:29.000Z","updated":"2019-09-06T08:53:18.240Z","comments":true,"path":"前端/前端优化/","link":"","permalink":"https://sexyphoenix.github.io/前端/前端优化/","excerpt":"","text":"前言 基本HTML加载，需要 20ms 左右 Nginx配置，关闭 keepalive、etag、gzip、if_modified_since协议：HTTP/1.1 浏览器：Chrome 减少HTTP请求 加载未合并外部css，需要 35ms 左右 加载合并外部css，需要 25ms 左右 两个合并后的css，加载减少了10ms，如果将页面所有的css、js、图片（CSS sprites ）合并，减少的时间将很可观。 DOM以及CSS 上图是浏览器解析HTML和渲染树之间的流程。浏览器在获取到HTML页面后开始解析页面，解析到head标签后，发现外部CSS，会异步发出请求，CSS获取后，解析CSS。 HTML解析后生成DOM Tree，CSS解析后生成CSSOM Tree， 两者结合开始渲染树。1、首屏的页面要快速的渲染出来，CSS最好放在页面头部。同时有多个css文件的时候，也要将基本样式放在其他样式之前加载（边获取边渲染）。2、HTML以及CSS的元素层级要尽量少,加快页面渲染。3、对于首页，可以将基本样式内联放在头部。（快速渲染，灵活应用） JS 上图是浏览器解析流程，蓝色是样式解析，黄色是JS脚本执行，顺序执行。JS脚本执行会阻塞样式或DOM解析1、JS脚本放在页面下面，防止阻塞页面渲染。2、不要在JS里执行长时间的程序。3、减少JS对DOM的操作，可减少浏览器的回流以及重绘。 Accept-Encoding: gzip 前言Nginx配置中关闭了gzip功能，页面获取的Size如上：1.9k、1013b、42kb。。。 Nginx配置开启gzip，设置css、js压缩类型。页面获取的Size如上：650b、505b。。。1、各种资源的Size能减少到之前的30% ~ 50%；2、正式环境下，一般css、js等都是压缩过的，不要在nginx中开启这些类型的压缩。3、gzip这些算法一般是对文件重复的字符做优化，如果文件很小以及重复很少，很可能会发现压缩后的反而比之前还大。 Connection: keep-alive 前言Nginx配置中关闭了keep-alive，看上图：1、Connection ID 不可以复用，连接都是开启了新HTTP，进行了重复性的DNS Lookup（方框中绿色长度代表DNS执行时间）、Initial connection（方框中黄色长度代表TCP三次握手时间）。2、上图右边花了三个方框，浏览器对并发请求有连接限制，Chrome是6个，可以看出第二、三框前期进行了长时间的等待（等后期讲应用层面的优化再来解决这个问题）。 Nginx配置中设置 keepalive_timeout 60s， 开启 keep-alive，看上图：1、Connection ID可复用，右边DNS Lookup，Initial connection时间消耗大幅度减少。 缓存 前言Nginx配置中，设置了 Last-Modified 为空，etag off、if_modified_since off 这样可以使客户端彻底不使用缓存。上图可以看出资源都走了网络请求。缓存有两种;1、验证性缓存（ETag、Last-Modified）2、非验证性缓存（Cache-Control、Expires） Cache-Control Cache-Control 特点是一但有效期内，就不会向服务器发送请求。虽然它是非验证性缓存，但其实我更想将它理解为实现缓存的一种机制。1、缓存请求主要指令：max-age= //内用客户端缓存，相对于客户端第一次访问服务器的时间max-age=0 //要向服务器发送请求验证,是否使用缓存文件no-cache //要向服务器发送请求验证,是否使用缓存文件no-store //禁止缓存指令是单向的，在响应中不一定包含相同的指令 2、缓存响应主要指令no-store //禁止缓存 服务器设置（add_header Cache-Control no-store）no-cache //要向服务器发送请求验证,是否使用缓存文件max-age= //内用客户端缓存，相对于客户端第一次访问服务器的时间must-revalidate //本地客户端过期，必须访问服务器 Nginx配置 add_header cache-control ‘public,max-age=10’，文件在10s内从缓存中取出，时间相对于请求时间Date，10s后刷新页面重新向服务器请求文件。 Last-Modified / If-Modified-Since Last-Modified 和 If-Modified-Since 是一组。Last-Modified ： 服务器发送给客户端， 代表文件最后一次修改的时间。If-Modified-Since ： 客户端发送给服务器， 此值第一次访问页面时 request 中是不存在此header的，第二次访问时，由第一次获取到的 Last-Modified 值 赋给 If-Modified-Since。 服务器获取到客户端传递的If-Modified-Since和 Last-Modified 比较，相同返回304，不同返回200Nginx配置中设置：add_header cache-control ‘no-cache,must-revalidate’ //强制向服务器验证，是否使用缓存。if_modified_since exact //开启 if_modified_since 验证#add_header Last-Modified “”; //注释对 Last-Modified 的设置 在上图：1、Last-Modified 和 If-Modified-Since 值相等，从客户端缓存中获取。2、304状态码，服务器只返回头信息，不返回主体内容。 修改其中的 jquery-ui.css 文件 Last-Modified 和 If-Modified-Since 值不相等，重新获取，状态码 200，其他文件的状态码仍然是 304。 修改Nginx配置 add_header cache-control ‘max-age=5’max-age= 和 Last-Modified 、 If-Modified-Since 并用， 5s内读取客户端缓存，5s后向服务器发送请求，再对比 Last-Modified 和 If-Modified-Since 发现相等， 返回状态码 304 ，直接读取客户端缓存。 ETag / If-None-Match ETag 和 If-None-Match 也是一组。ETag ： 服务器发送给客户端， 服务器根据文档内容生成一串字符。If-None-Match ： 客户端发送给服务器， 此值第一次访问页面时 request 中是不存在此header的，第二次访问时，由第一次获取到的 ETag 值 赋给 If-None-Match。 服务器获取到客户端传递的 If-None-Match 和 ETag 比较，相同返回304，不同返回200Nginx配置中设置：add_header cache-control ‘no-cache,must-revalidate’ //强制向服务器验证，是否使用缓存。etag on //开启 etag 验证add_header Last-Modified “”; //关闭 Last-Modified设置if_modified_since off; 在上图：1、ETag 和 If-None-Match 值相等，从客户端缓存中获取。2、304状态码，服务器只返回头信息，不返回主体内容。 文件修改后，会重新生成Etag，返回200 状态码，这个和 Last-Modified一样就不说了。 最后说一下既然存在 Last-Modified， 为什么还要 Etag。1、 Last-Modified只能精确到s级，如果在ms内修改了文件，文件的 Last-Modified 就是一样，客户端就不能获取信息的文件。2、在分布式环境下，动态生成的页面就会存在时间不一致，但内容却没有修改。导致缓存有问题。 讲在最后 关于前端性能优化，本篇也就讲到这了，但这些也只是比较基础的内容，在实际使用中根据环境也会有很多问题。关于没有讲到的，比如减少重定向、Cookie优化、DNS（HTML5 预加载）、CDN、图片优化、JS细节优化、Ajax优化、分布式下各种参数可能会出现的问题、开启gzip和Etag冲突等等，感兴趣的可以自己在网上搜一搜，看一看。最后，给大家放两张比较有意思的图（网上找的），大家可以研究研究。performance.timing 浏览器执行流程","categories":[{"name":"前端","slug":"前端","permalink":"https://sexyphoenix.github.io/categories/前端/"}],"tags":[],"keywords":[{"name":"前端","slug":"前端","permalink":"https://sexyphoenix.github.io/categories/前端/"}]},{"title":"消息中心（实现）","slug":"消息中心（实现）","date":"2019-09-06T08:31:35.000Z","updated":"2019-09-06T08:34:39.141Z","comments":true,"path":"PHP/消息中心（实现）/","link":"","permalink":"https://sexyphoenix.github.io/PHP/消息中心（实现）/","excerpt":"","text":"前言 在上一章主要梳理了信息中心的运行机制，了解了服务端是如何把更新的信息主动推送给客户端的，接下来我会介绍下消息中心是如何搭建的以及现在可能会出现问题。 背景知识 laravel 队列运行机制：队列消息中心实现逻辑：消息中心（逻辑） 消息中心1.设置配置.env文件 12BROADCAST_DRIVER = redisQUEUE_DRIVER=redis config/app.php 1App\\Providers\\BroadcastServiceProvider::class //加载 2. 设置路由//设置用户频道，以便客户端监听。为了方便，直接返回true 123Broadcast::channel(&apos;todoevent.updated.*&apos;, function($user, $userId)&#123; return true;&#125;); 3. 添加predis依赖1composer require predis/predis 4. 创建事件类1php artisan make:event TodoEventUpdated //默认是不实现ShouldBroadcast接口 12345678910111213141516171819202122232425 class TodoEventUpdated implements ShouldBroadcast &#123; use InteractsWithSockets, SerializesModels; public $event; public function __construct($event) &#123; $this-&gt;event= $event; &#125; //添加私人频道，于路由设置一致 public function broadcastOn() &#123; if($this-&gt;event) &#123; return new PrivateChannel(&apos;todoevent.updated.&apos; . $this-&gt;event-&gt;get(&apos;user_id&apos;)); &#125; &#125; //可对事件内容作修改。返回客户端需要的内容 // public function broadcastWith() // &#123; // &#125;&#125; 5. 测试广播已到redisHomeController.php 12345use App\\Events\\TodoEventUpdated;public function index()&#123; broadcast(new TodoEventUpdated(collect([&apos;user_id&apos; =&gt; 8164]))); &#125; 执行home/index后，查看redis客户端 12&quot;SELECT&quot; &quot;0&quot;&quot;RPUSH&quot; &quot;queues:default&quot; &quot;&#123;event&#125;&quot; event已经写入redis，代表redis连接成功 6. 开启消费进程1php artisan queue:work //发现事件已被消费 12Processing: App\\Events\\TodoEventUpdatedProcessed: App\\Events\\TodoEventUpdated 7. 安装 laravel-echo-server，订阅redis1cnpm install -g laravel-echo-server 安装成功后，移步到项目下 12laravel-echo-server init //初始化laravel-echo-server start //启动 laravel-echo-server 已成功的监听了 redis 。注：laravel-echo-server 运行在6001端口，如果你用的是homestead等集成环境，请查看端口是否开启 8. 安装socket.io-client、laravel-echo12cnpm i --save socket.io-clientcnpm i --save laravel-echo 打开/resources/assets/js/bootstrap.js，会出现下面代码 123456import Echo from &apos;laravel-echo&apos;window.Pusher = require(&apos;pusher-js&apos;);window.Echo = new Echo(&#123; broadcaster: &apos;pusher&apos;, key: &apos;your-pusher-key&apos;&#125;); 改造为： 12import Echo from &apos;laravel-echo&apos;window.LaravelEcho= Echo; 编译 1cnpm run dev 以上的事件广播都搭建完了。下面让第三方应用可以监听吧 9. 第三方应用监听1234567891011&lt;script src=&quot;http://xxx:6001/socket.io/socket.io.js&quot;&gt;&lt;/script&gt; //引入socket.io客户端&lt;script src=&quot;text/javascript&quot;&gt; window.Echo = new LaravelEcho(&#123; broadcaster: &apos;socket.io&apos;, host: &apos;http://xxx:6001&apos; &#125;); Echo.private(&apos;todoevent.updated.8164&apos;) .listen(&apos;TodoEventUpdated&apos;, function(event) &#123; console.log(event); //获取的信息 &#125;);&lt;/script&gt;","categories":[{"name":"PHP","slug":"PHP","permalink":"https://sexyphoenix.github.io/categories/PHP/"}],"tags":[],"keywords":[{"name":"PHP","slug":"PHP","permalink":"https://sexyphoenix.github.io/categories/PHP/"}]},{"title":"消息中心（逻辑）","slug":"消息中心（逻辑）","date":"2019-09-06T08:31:19.000Z","updated":"2019-09-06T08:34:03.478Z","comments":true,"path":"PHP/消息中心（逻辑）/","link":"","permalink":"https://sexyphoenix.github.io/PHP/消息中心（逻辑）/","excerpt":"","text":"前言 在构建web应用的时候，很多功能需要我们即时更新信息。当服务器上的数据被更新后，能够在客户端即时且无刷新的同步信息。为了解决这个问题，出现了websocket协议。 它可以使服务器和客户端一直保持通信，且通信是双向的。而Laravel的事件广播让Websocket获取事件变的更加方便。在我们的项目中（消息中心）用的就是Laravel的事件广播机制，接下来我会介绍下消息中心是如何运行的。 背景知识 WebService是一种跨编程语言和跨操作系统平台的远程调用技术，可以让客户端调用服务端上面的方法。消息中心通过WebService技术，添加或者取消代办事件。消息中心的事件广播是基于redis驱动的同步队列。socket.io是nodejs对websocket协议的封装。socket.io分为客户端和服务端，通过websocket协议保持浏览器与服务器的双向通信。laravel-echo-server 使用 socket.io 实现了Laravel 广播的服务端。laravel-echo 是Laravel广播的客户端，支持socket.io 和 pusher，消息中心用的是socket.ioredis Pub/Sub ： redis使用字典这种数据类型实现了发布/订阅的功能，laravel-echo-server就是redis Pub/Sub 的订阅者，消费者进程向redis里面发布了一条事件，redis 通过Pub/Sub功能通知了laravel-echo-server。 消息广播 现在的代办功能其实分成了两个部分，第一个部分放在了新oa的event应用下，用于对事件信息的持久化以及对信息中心的通知。第二部分放在了信息中心系统（focusmessage），用于对用户客户端即时且无刷新的信息同步。 1. 代办事件添加/删除第三应用通过调用web api 将信息推送到webservice Service（A），Service端先对第三应用的ip做了判断，看是否在允许的地址里。通过，再对账号以及密码进行验证。两种验证只要有一种失败，就会返回验证失败信息。验证通过后，信息进行持久化，写入数据库（B）以及更新日历（C），最后通知信息中心（D）。 2. 消息中心消息中心获取到信息的更新，会对信息的来源ip做白名单判断，失败返回403，通过后广播这条信息，也就是推送到redis主队列中（E），消费者进程进行publish（F），laravel-echo-server接受到发布的信息后（G），通知socket.io service。 因为socket.io service和socket.io client 一直保持通信且一直监控相关频道中的事件，符合此频道中的事件后，发送给 socket.io client 后，laravel-echo 通过 socket.io 接受到信息，解析出信息后，显示给用户。 结尾 本来也想在本章写下如何搭建我们的信息中心，但想想感觉搭建的内容还是比较多的，就将如何搭建内容放在了下一章。此章就对待办事项的逻辑做了一下梳理。","categories":[{"name":"PHP","slug":"PHP","permalink":"https://sexyphoenix.github.io/categories/PHP/"}],"tags":[],"keywords":[{"name":"PHP","slug":"PHP","permalink":"https://sexyphoenix.github.io/categories/PHP/"}]},{"title":"API认证系统Passport","slug":"API认证系统Passport","date":"2019-09-06T08:31:10.000Z","updated":"2019-09-06T08:33:36.666Z","comments":true,"path":"PHP/API认证系统Passport/","link":"","permalink":"https://sexyphoenix.github.io/PHP/API认证系统Passport/","excerpt":"","text":"安装1composer require laravel/passport=~4.0 notes: 1）确保系统安装unzip、zip等命令。 2）composer 安装出现 Authentication required (packagist.phpcomposer.com) 问题，修改composer.json 中的源，repositories.packagist.url = https://packagist.laravel-china.org 。 注册服务提供者在config/app.php的providers 数组中加入 Laravel\\Passport\\PassportServiceProvider::class 迁移数据库1php artisan migrate //生成用于存储客户端和令牌的数据表 生成加密健1php artisan passport:install 1、生成oauth-private.key（用于构建认证服务器），oauth-public.key（用于构建资源服务器）2、oauth_clients数据库生成「个人访问」客户端和「密码授权]两条数据。 配置Passport（参考官方文档）在Model中，我们需要增加 HasApiTokens class在AuthServiceProvider中， 增加 “Passport::routes()”在 auth.php中， 更改 api 认证方式为passport 申请客户端以及私人访问令牌 （两种方式）1. 命令形式（不方便客户注册）1php artisan passport:client 2. Passport Vue 组件1php artisan vendor:publish --tag=passport-components //发布 Passport Vue，组件位于resources/assets/js/components下 //注册到resources/assets/js/app.js 文件，记得要放在new Vue上面 1234567891011121314Vue.component( &apos;passport-clients&apos;, require(&apos;./components/passport/Clients.vue&apos;));Vue.component( &apos;passport-authorized-clients&apos;, require(&apos;./components/passport/AuthorizedClients.vue&apos;));Vue.component( &apos;passport-personal-access-tokens&apos;, require(&apos;./components/passport/PersonalAccessTokens.vue&apos;)); //编译前端资源 12npm install //此处报错，移步larravel Mix文档npm run dev 编译后资源放在public/js/app.js下 //组件放入应用模板（记得引入编译后的app.js） 123&lt;passport-clients&gt;&lt;/passport-clients&gt;&lt;passport-authorized-clients&gt;&lt;/passport-authorized-clients&gt;&lt;passport-personal-access-tokens&gt;&lt;/passport-personal-access-tokens&gt; 以上认证服务器都已经搭建完成 第三方应用实现登录1. 申请客户端回调地址 http://third.plat.goods/dew/sso 申请授权码和访问令牌//获取授权码 code （第一次交互） 123456789$query = http_build_query(array( &apos;client_id&apos; =&gt; 3, &apos;redirect_uri&apos; =&gt; &apos;http://third.plat.goods/dew/sso&apos;, //地址必须为上面的回调地址 &apos;response_type&apos; =&gt; &apos;code&apos;, //固定值 &apos;scope&apos; =&gt; &apos;&apos;, &apos;state&apos; =&gt; urlencode(&apos;http://laravel.plat.goods/user&apos;) //可以放用户访问的地址。));return redirect(&apos;http://laravel.plat.goods/oauth/authorize?&apos;.$query); ///laravel.plat.goods为上面认证服务器 //获取访问令牌 access token 以及向资源服务器请求用户信息授权后会重定向回调地址 12Route::get(&apos;/dew/sso&apos;, &apos;SSOController@callback&apos;); //路由文件里添加php artisan make:controller SSOController //创建文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116&lt;?phpnamespace App\\Http\\Controllers;use App\\Models\\User;use GuzzleHttp\\Client;use Illuminate\\Http\\Request;use Illuminate\\Support\\Facades\\Log;class SSOController extends Controller&#123; protected $http; public function __construct() &#123; $this-&gt;http = new Client(); &#125; /** * 获取授权码后的回调URL * @param Request $request * @return \\Illuminate\\Http\\RedirectResponse */ public function callback(Request $request) &#123; $token = $this-&gt;token($request); //第二次交互 $login = $this-&gt;login($token);//第三次交互 if($login)&#123; if($request_url = $request-&gt;input(&apos;state&apos;, null))&#123; $request-&gt;session()-&gt;put(&apos;url.intended&apos;, urldecode($request_url)); &#125; return redirect()-&gt;intended(); //跳转到 http://laravel.plat.goods/user &#125;else&#123; return redirect()-&gt;to(&apos;http://laravel.plat.com/home/public/login&apos;); //服务提供商网站必须登录 &#125; &#125; /** * 获取access token * @param $request * @return array|mixed */ protected function token($request) &#123; $code = $request-&gt;code; if($code) &#123; try &#123; $response = $this-&gt;http-&gt;post(&apos;http://laravel.plat.goods/oauth/token&apos;, [ &apos;form_params&apos; =&gt; [ &apos;grant_type&apos; =&gt; &apos;authorization_code&apos;, //固定值 &apos;client_id&apos; =&gt; 3, &apos;client_secret&apos; =&gt; &apos;UihXNHoSqohdtQ8Js6Av7AOyk3GBNB9rJziDPaWf&apos;, &apos;redirect_uri&apos; =&gt; &apos;http://third.plat.goods/dew/sso&apos;, &apos;code&apos; =&gt; $code, ], ]); $response_data = json_decode((string)$response-&gt;getBody(), true); return $response_data; &#125; catch (\\Exception $e) &#123; Log::error(&apos;get token by code failed: &apos;.$code.&apos; - &apos;.$e-&gt;getMessage().&apos; - &apos;.$e-&gt;getTraceAsString()); return []; &#125; &#125;else&#123; return []; &#125; &#125; /** * 通过token获取用户信息，并进行登录操作 * @param $token * @return bool */ protected function login($token) &#123; if(empty($token)) return false; $access_token = $token[&apos;access_token&apos;]; try &#123; // 资源服务器和认证服务器放在了一起，可以独立。 $response = $this-&gt;http-&gt;request(&apos;GET&apos;, &apos;http://laravel.plat.goods/api/user&apos;, [ &apos;headers&apos; =&gt; [ &apos;Accept&apos; =&gt; &apos;application/json&apos;, &apos;Authorization&apos; =&gt; $token[&apos;token_type&apos;] . &apos; &apos; . $access_token, ] ]); $users_body = $response-&gt;getBody(); $data = json_decode($users_body, true); if($data) &#123; $user = new User($data); //because of employee_id is guarded $user-&gt;setAttribute($user-&gt;getKeyName(), $data[&apos;employee_id&apos;]); //login user in my system auth()-&gt;login($user, false); return true; &#125;else&#123; return false; &#125; &#125;catch (\\Exception $e)&#123; Log::error(&apos;get user failed by access_token:&apos;.$access_token.&apos;|&apos;.$e-&gt;getMessage()); return false; &#125; &#125;&#125; //设置资源文件 12Route::middleware(&apos;auth:api&apos;)-&gt;get(&apos;/user&apos;, &apos;UserController@user&apos;); //routes/api.php文件中设置php artisan make:controller UserController //创建文件 1234567class UserController extends Controller&#123; public function user(Request $request) &#123; return $request-&gt;user(); &#125;&#125;","categories":[{"name":"PHP","slug":"PHP","permalink":"https://sexyphoenix.github.io/categories/PHP/"}],"tags":[],"keywords":[{"name":"PHP","slug":"PHP","permalink":"https://sexyphoenix.github.io/categories/PHP/"}]},{"title":"OAuth2.0","slug":"OAuth2-0","date":"2019-09-06T08:31:01.000Z","updated":"2019-09-06T08:33:08.532Z","comments":true,"path":"PHP/OAuth2-0/","link":"","permalink":"https://sexyphoenix.github.io/PHP/OAuth2-0/","excerpt":"","text":"OAuth 2.0定义了四种授权方式 授权码模式（authorization code） 简化模式（implicit） 密码模式（resource owner password credentials） 客户端模式（client credentials）授权码模式是功能最完整、流程最严密的授权模式，本篇也是主要去理解这种模式 授权码模式大概分为 5 个步骤 客户端（Client）向服务提供商（HTTP service）申请创建客户端（Client_id、Client_Secret）。 用户（Resource Owner）通过浏览器（User Agent）打开后,跳转到授权页，客户端要求用户授权。 用户同意给予客户端授权，返回授权码（Code）。 客户端通过授权码，向认证服务器（Authorization server）申请令牌（Access Token）。 客户端通过令牌，向资源服务器（Resource server）获取资源。 1. 获取Code12345response_type：表示授权类型，必选项，此处的值固定为&quot;code&quot;client_id：表示客户端的ID，必选项redirect_uri：表示重定向URL，可选项scope：表示申请的权限范围，可选项state：表示客户端的当前状态，可以指定任意值，认证服务器会原封不动地返回这个值。 2. 返回Code（用户授权通过后返回到重定向URL）12code：表示授权码，必选项。state：如果客户端的请求中包含这个参数，认证服务器的回应也必须一模一样包含这个参数。 3. 客户端向认证服务器申请Access Token12345grant_type：表示使用的授权模式，必选项，此处的值固定为&quot;authorization_code&quot;。code：表示获得的授权码，必选项。redirect_uri：表示重定向URI，必选项，且必须与上面中的该参数值保持一致。client_id：表示客户端ID，必选项。client_secret ： 表示客户端密钥，必选项。 4. 认证服务器返回Access Token12345access_token：表示访问令牌，必选项。token_type：表示令牌类型，该值大小写不敏感，必选项，可以是bearer类型或mac类型。expires_in：表示过期时间，单位为秒。如果省略该参数，必须其他方式设置过期时间。refresh_token：表示更新令牌，用来获取下一次的访问令牌，可选项。scope：表示权限范围，如果与客户端申请的范围一致，此项可省略。 5. 向资源服务器获取信息12headers.Accept ： media类型，固定值 “application/json”headers.Authorization 授权，值为返回的token_type + 空格 + access_token 疑问1. 获取code时，只传递了clent_id，redirect_url等值，服务提供商是怎么知道是哪个用户授权？ 授权时，你已经登录了服务提供商的网站或者会要求你登录。 2. 客户端是怎么知道你已经授权？ 授权请求发出后，浏览器得到的是一个http的重定向响应，这个地址是你的redirect_url，同时返回code值 3. 为什么要设置获取code后再去获取access_token 是为了安全性，直接通过重定向传回access_token，但是HTTP 302是不安全的， 攻击者有可能会获取到access_token，而code不能获取资源，即使被截取也没什么用，client通过HTTPS以及密钥来获取access_token，以保证安全。 为什么不直接用HTTPS重定向回client不是所有client都支持HTTPS，为了通用性 和安全性，才衍生出来这么一个code。","categories":[{"name":"PHP","slug":"PHP","permalink":"https://sexyphoenix.github.io/categories/PHP/"}],"tags":[],"keywords":[{"name":"PHP","slug":"PHP","permalink":"https://sexyphoenix.github.io/categories/PHP/"}]},{"title":"队列","slug":"队列","date":"2019-09-06T08:30:52.000Z","updated":"2019-09-06T08:32:21.627Z","comments":true,"path":"PHP/队列/","link":"","permalink":"https://sexyphoenix.github.io/PHP/队列/","excerpt":"","text":"前言 Laravel的队列可以用在轻量级的队列需求中。比如我们系统中的短信、邮件等功能，这些功能有一些普遍的特征，异步、重试、并发控制等。Laravel现在主要支持的队列服务有Null、Sync、Database、Redis、Beanstalkd、Sqs。在我们的项目中（消息中心、人事）用的主要是redis，接下来我会介绍下队列基于redis驱动的运行机制。 背景知识 Laravel启动后，加载config/app.php的providers数组中的服务提供者 QueueServiceProvider，在队列服务提供者中，已经注册了一系列相关服务。 在.env配置文件中，我们设置了QUEUE_DRIVER为redis，系统启动后会自动生成Redis连接，同时注册了Work消费者，队列监听器以及错误服务。在queue.php和database.php配置中都默认了相关的redis设置。 任务调度 调度如下： laravel 的队列服务由两个进程控制，一个是事件生产者（黑线），一个是事件消费者（黄线）。有三个队列，主队列 queues：default（下面default代替），延时队列 queues：default：delayed（下面delayed代替），待处理队列 queues：default：reserved（下面reserved代替）。所有的队列事件都由事件消费者去消费主队列中的事件。（队列名称default在queue.php中被定义） 1. 事件触发：dispatch(new Event())；事件触发后，dispatch判断Event是否继承队列类，是，将事件分发到队列执行流程中。队列执行流程会根据Event的延时属性判断，否，将Event放到即时处理queues：default 队列中，是，将Event放入延时 queues：default：delayed 队列中。 2. 消费：php artisan queue:work图中A、B、C、D为消费者进程依次执行步骤，淡黄色背景的代码备注都为对队列的操作命令（predis 实现redis api），每个备注里面对事件的操作要么一起成功，要么一起失败（Lua脚本）。 3. A： 对delayed、default队列操作A1：取出小于当前时间的（时间戳）所有Event，赋给val，删除delayed队列中0到val.length长度的Event（redis的有序集合有一个分数，有序集合根据这个分数从小到大排序，这里的时间戳就是分数）。A2：将上面获取的Event，放入到主队列 default。 4. B： 对reserved、default队列操作B1：取出小于当前时间的所有Event，赋给val，删除reserved队列中0到val长度的Event。B2：将上面获取的Event，放入到主队列 default。 5. C： 对default、reserved队列操作C1：取出主队列中的所有Event。C2：将Event放入reserved，且记录Event的执行次数（保留Event，由D执行后，根据Event执行结果处理这些Event）。 6. D： 处理Event（由C步骤得到的Event，交给D）根据得到的Event依次执行（也就是通知监听这个Event的所有监听者），同时删除reserved队列的相对应的Event（无论执行失败还是成功），如果执行失败会将任务放入reserved队列中，执行时间为1540097000（1540096910 + 90，90为设置的延时时间），以便下次执行。 结尾 以上就是Laravel队列所有的执行流程，当然里面包括执行失败的错误处理、如何通知监听者等细节都没讲，大家可以自行分析代码理解。","categories":[{"name":"PHP","slug":"PHP","permalink":"https://sexyphoenix.github.io/categories/PHP/"}],"tags":[],"keywords":[{"name":"PHP","slug":"PHP","permalink":"https://sexyphoenix.github.io/categories/PHP/"}]},{"title":"用户认证","slug":"用户认证","date":"2019-09-06T08:23:46.000Z","updated":"2019-09-06T08:29:51.672Z","comments":true,"path":"PHP/用户认证/","link":"","permalink":"https://sexyphoenix.github.io/PHP/用户认证/","excerpt":"","text":"前期准备Laravel的权限配置文件位于 config/auth.php，Laravel的认证组件由“guards”和“providers”组成，Guard 通过 session 来维护用户登录的状态。Provider 是登录以及访问页面的时候获取用户的信息。本篇主要讲的是如何自定义Provider ，获取用户信息。 config/auth.php文件Laravel提供了两种guard，web以及api，采取默认的web认证。在guards的web中，用了users提供者。接下来就需要注意了，我们自定义了服务提供者，就需要换到新的providers。首先，定义一个使用新驱动的provider： 123456 &apos;providers&apos; =&gt; [ &apos;users&apos; =&gt; [ &apos;driver&apos; =&gt; &apos;Focus&apos;, //名称自定义，这里为Focus &apos;model&apos; =&gt; App\\Models\\User::class, //Model放在Models文件夹下 ],], Notes: 默认提供了两种驱动database和eloquent，而这两种的提供者分别是DatabaseUserProvider和EloquentUserProvider，都处于laravel\\framework\\src\\Illuminate\\Auth文件夹下，实现了UserProvider，我们自定义的 Focus provider 也将实现UserProvider。 生成路由和视图1php artisan make:auth //命令可快速生成认证所需要的路由和视图 Http/Controllers 和 resources/views下会相应生成控制器和视图默认用的Email，我们用username LoginController：//添加此方法，返回username 123public function username()&#123; return &apos;username&apos;;&#125; login.blade.php：将邮箱改为域账号，email 改为username 数据库修改.env配置中的数据库信息。 1php artisan make:model Models/User // 使用命令创建User模型 默认User是继承Model的，需要修改。 123456789use Illuminate\\Foundation\\Auth\\User as Authenticatable; //引入Authenticatableclass User extends Authenticatable&#123; protected $table = &apos;employee&apos;; protected $primaryKey = &apos;employee_id&apos;; //can set all fields to user model protected $guarded = [];&#125; 为什么要引入Authenticatable呢，是因为Authenticatable实现了Illuminate\\Contracts\\Auth\\Authenticatable接口，而FocusUserProvider 需要用到接口的实现。 创建扩展在app下新建 Extensions/FocusUserProvider 文件，参考DatabaseUserProvider和EloquentUserProvider，实现UserProvider： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374namespace App\\Extensions;use App\\Services\\LdapValidator;use Illuminate\\Support\\Str;use Illuminate\\Contracts\\Auth\\UserProvider;use \\Illuminate\\Contracts\\Auth\\Authenticatable; class LaravelUserProvider implements UserProvider &#123; protected $model; public function __construct($model) &#123; $this-&gt;model = $model; &#125; //登录成功后，通过此方法获取用户信息，返回匹配该ID的 Authenticatable 实现 public function retrieveById($identifier) &#123; //此处可以将信息放入缓存，缓解数据库压力。 $model = $this-&gt;createModel(); return $model-&gt;newQuery() -&gt;where($model-&gt;getAuthIdentifierName(), $identifier) -&gt;first(); &#125; public function retrieveByToken($identifier, $token) &#123; &#125; public function updateRememberToken(Authenticatable $user, $token) &#123; &#125; //该方法可以根据账号名去查询数据库是否存在匹配的账号 public function retrieveByCredentials(array $credentials) &#123; if (empty($credentials)) &#123; return; &#125; // First we will add each credential element to the query as a where clause. // Then we can execute the query and, if we found a user, return it in a // Eloquent User &quot;model&quot; that will be utilized by the Guard instances. $query = $this-&gt;createModel()-&gt;newQuery(); foreach ($credentials as $key =&gt; $value) &#123; if (! Str::contains($key, &apos;password&apos;)) &#123; $query-&gt;where($key, $value); &#125; &#125; return $query-&gt;first(); &#125; //该方法可以验证密码是否正确，因为我们是ldap登录，可以在此验证域账号 public function validateCredentials(Authenticatable $user, array $credentials) &#123; //LdapValidator类是为了验证域密码的，放在了app/Services，在上面已经引入 $Ldap = new LdapValidator($user-&gt;username, $credentials[&apos;password&apos;]); return $Ldap-&gt;validatePassword(); &#125; /** * Create a new instance of the model. * * @return \\Illuminate\\Database\\Eloquent\\Model */ public function createModel() &#123; $class = &apos;\\\\&apos;.ltrim($this-&gt;model, &apos;\\\\&apos;); return new $class; &#125;&#125; 注册提供者Laravel 提供了AuthServiceProvider， 我们可以在这里注册。 12345678910public function boot()&#123; $this-&gt;registerPolicies(); //Focus为auth.php里面定义的驱动 Auth::provider(&apos;Focus&apos;, function($app, array $config)&#123; return new FocusUserProvider ($config[&apos;model&apos;]); &#125;);&#125; 下面就可以访问http://你的域名/login 登录系统","categories":[{"name":"PHP","slug":"PHP","permalink":"https://sexyphoenix.github.io/categories/PHP/"}],"tags":[],"keywords":[{"name":"PHP","slug":"PHP","permalink":"https://sexyphoenix.github.io/categories/PHP/"}]},{"title":"生命的意义","slug":"生命的意义","date":"2019-09-06T08:11:47.000Z","updated":"2019-09-06T08:21:47.640Z","comments":true,"path":"PHP/生命的意义/","link":"","permalink":"https://sexyphoenix.github.io/PHP/生命的意义/","excerpt":"","text":"我们该如何找出生命的意义呢？ 我（霍金）认为答案非常清楚 意义本身不过就是 每个人在大脑里建构的现实模型其中一部分 生命的意义为何 全由你来选择 个人来说 我很乐意 我们每一个人都赋予宇宙意义 如宇宙学家卡尔·萨根所说 我们是宇宙对自己的省思 意义只能存在于人类心智的架构内 如此一来 生命的意义就不在外面某处 而在我们的脑海里 就许多方面来说 我们也因此成了万物之灵","categories":[],"tags":[],"keywords":[]}]}